{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd88ef8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data path:      /Users/seanice/Desktop/Bootcamp/bootcamp_Linxi_Pan/homework/homework5/data/raw\n",
      "Processed data path: /Users/seanice/Desktop/Bootcamp/bootcamp_Linxi_Pan/homework/homework5/data/processed\n"
     ]
    }
   ],
   "source": [
    "# Cell 0 — Environment Setup\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get paths from .env (fallback to defaults if not set)\n",
    "DATA_DIR_RAW = Path(os.getenv(\"DATA_DIR_RAW\", \"data/raw\"))\n",
    "DATA_DIR_PROCESSED = Path(os.getenv(\"DATA_DIR_PROCESSED\", \"data/processed\"))\n",
    "\n",
    "# Ensure folders exist\n",
    "DATA_DIR_RAW.mkdir(parents=True, exist_ok=True)\n",
    "DATA_DIR_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Raw data path:     \", DATA_DIR_RAW.resolve())\n",
    "print(\"Processed data path:\", DATA_DIR_PROCESSED.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd95ba57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (10, 3)\n",
      "Columns: ['category', 'value', 'date']\n",
      "NA counts:\n",
      " category    0\n",
      "value       0\n",
      "date        0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>value</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>10</td>\n",
       "      <td>2025-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>15</td>\n",
       "      <td>2025-08-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>12</td>\n",
       "      <td>2025-08-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>18</td>\n",
       "      <td>2025-08-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C</td>\n",
       "      <td>25</td>\n",
       "      <td>2025-08-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category  value        date\n",
       "0        A     10  2025-08-01\n",
       "1        B     15  2025-08-02\n",
       "2        A     12  2025-08-03\n",
       "3        B     18  2025-08-04\n",
       "4        C     25  2025-08-05"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 1 — Load Starter Data\n",
    "\n",
    "# Load CSV into DataFrame\n",
    "df = pd.read_csv(\"starter_data.csv\")\n",
    "\n",
    "# Basic info\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"NA counts:\\n\", df.isna().sum())\n",
    "\n",
    "# Preview first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee157c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CSV to: data/raw/sample_20250820-1831.csv\n",
      "Saved Parquet to: data/processed/sample_20250820-1831.parquet\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — Save DataFrame to CSV and Parquet\n",
    "\n",
    "import time\n",
    "import fastparquet\n",
    "\n",
    "\n",
    "# Generate timestamp for reproducible filenames\n",
    "ts = time.strftime(\"%Y%m%d-%H%M\")\n",
    "\n",
    "# Save to CSV in raw directory\n",
    "csv_path = DATA_DIR_RAW / f\"sample_{ts}.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(\"Saved CSV to:\", csv_path)\n",
    "\n",
    "# Save to Parquet in processed directory\n",
    "df.to_parquet(parquet_path, index=False, engine=\"fastparquet\")\n",
    "parquet_path = DATA_DIR_PROCESSED / f\"sample_{ts}.parquet\"\n",
    "print(\"Saved Parquet to:\", parquet_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c604d25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: {'shape_equal': True, 'columns_equal': True, 'dtypes_equal': True}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>value</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>10</td>\n",
       "      <td>2025-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>15</td>\n",
       "      <td>2025-08-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>12</td>\n",
       "      <td>2025-08-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>18</td>\n",
       "      <td>2025-08-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C</td>\n",
       "      <td>25</td>\n",
       "      <td>2025-08-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category  value        date\n",
       "0        A     10  2025-08-01\n",
       "1        B     15  2025-08-02\n",
       "2        A     12  2025-08-03\n",
       "3        B     18  2025-08-04\n",
       "4        C     25  2025-08-05"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3 — Reload and Validate\n",
    "\n",
    "# Reload CSV\n",
    "df_csv = pd.read_csv(csv_path)\n",
    "\n",
    "# Reload Parquet (engine fallback)\n",
    "try:\n",
    "    df_parquet = pd.read_parquet(parquet_path, engine=\"pyarrow\")\n",
    "except Exception:\n",
    "    df_parquet = pd.read_parquet(parquet_path, engine=\"fastparquet\")\n",
    "\n",
    "# Validation function\n",
    "def validate_dataframes(df1, df2):\n",
    "    results = {}\n",
    "    results[\"shape_equal\"] = df1.shape == df2.shape\n",
    "    results[\"columns_equal\"] = list(df1.columns) == list(df2.columns)\n",
    "    # Compare dtypes\n",
    "    dtypes_match = all(str(df1[c].dtype) == str(df2[c].dtype) for c in df1.columns)\n",
    "    results[\"dtypes_equal\"] = dtypes_match\n",
    "    return results\n",
    "\n",
    "# Run validation\n",
    "validation_results = validate_dataframes(df_csv, df_parquet)\n",
    "print(\"Validation results:\", validation_results)\n",
    "\n",
    "# Preview\n",
    "df_csv.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0eb2e6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CSV: data/raw/util_test.csv\n",
      "pyarrow failed: A type extension with name pandas.period already defined\n",
      "Saved Parquet with fastparquet: data/processed/util_test.parquet\n",
      "CSV shape: (10, 3)\n",
      "Parquet shape: (10, 3)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 — Utility Functions\n",
    "\n",
    "def write_df(df: pd.DataFrame, path: Path):\n",
    "    \"\"\"Save DataFrame to CSV or Parquet depending on suffix.\"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    suffix = path.suffix.lower()\n",
    "    try:\n",
    "        if suffix == \".csv\":\n",
    "            df.to_csv(path, index=False)\n",
    "            print(\"Saved CSV:\", path)\n",
    "        elif suffix == \".parquet\":\n",
    "            try:\n",
    "                df.to_parquet(path, index=False, engine=\"pyarrow\")\n",
    "                print(\"Saved Parquet with pyarrow:\", path)\n",
    "            except Exception as e1:\n",
    "                print(\"pyarrow failed:\", e1)\n",
    "                try:\n",
    "                    df.to_parquet(path, index=False, engine=\"fastparquet\")\n",
    "                    print(\"Saved Parquet with fastparquet:\", path)\n",
    "                except Exception as e2:\n",
    "                    print(\"Both pyarrow and fastparquet failed:\", e2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file extension: {suffix}\")\n",
    "    except Exception as e:\n",
    "        print(\"Write failed:\", e)\n",
    "\n",
    "def read_df(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load DataFrame from CSV or Parquet depending on suffix.\"\"\"\n",
    "    suffix = path.suffix.lower()\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File does not exist: {path}\")\n",
    "    \n",
    "    if suffix == \".csv\":\n",
    "        return pd.read_csv(path)\n",
    "    elif suffix == \".parquet\":\n",
    "        try:\n",
    "            return pd.read_parquet(path, engine=\"pyarrow\")\n",
    "        except Exception:\n",
    "            return pd.read_parquet(path, engine=\"fastparquet\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file extension: {suffix}\")\n",
    "\n",
    "# --- Test utilities ---\n",
    "test_csv = DATA_DIR_RAW / \"util_test.csv\"\n",
    "test_parquet = DATA_DIR_PROCESSED / \"util_test.parquet\"\n",
    "\n",
    "write_df(df, test_csv)\n",
    "write_df(df, test_parquet)\n",
    "\n",
    "df_loaded_csv = read_df(test_csv)\n",
    "df_loaded_parquet = read_df(test_parquet)\n",
    "\n",
    "print(\"CSV shape:\", df_loaded_csv.shape)\n",
    "print(\"Parquet shape:\", df_loaded_parquet.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577442c2",
   "metadata": {},
   "source": [
    "## Data Storage Documentation\n",
    "\n",
    "### Folder Structure\n",
    "- **data/raw/**  \n",
    "  Stores raw ingested data in CSV format.  \n",
    "- **data/processed/**  \n",
    "  Stores processed data in Parquet format.  \n",
    "\n",
    "The paths are defined in `.env` file as:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ee3498",
   "metadata": {},
   "source": [
    "\n",
    "### Formats Used\n",
    "- **CSV (Comma-Separated Values)**  \n",
    "  - Human-readable, universal support.  \n",
    "  - Used for raw storage to ensure transparency.  \n",
    "- **Parquet (Columnar Storage)**  \n",
    "  - Efficient for analytical workloads, compressed and faster for large-scale reads.  \n",
    "  - Used for processed storage to optimize space and performance.  \n",
    "\n",
    "### How the Code Reads/Writes\n",
    "- `write_df(df, path)` automatically chooses CSV or Parquet based on file extension.  \n",
    "- `read_df(path)` loads the file accordingly, with fallback between `pyarrow` and `fastparquet` engines for Parquet.  \n",
    "- Both functions ensure directories exist and give clear error messages if engines are missing.  \n",
    "\n",
    "### Validation\n",
    "- Shapes and columns between CSV and Parquet are compared.  \n",
    "- Critical columns’ dtypes are checked to ensure consistency.  \n",
    "\n",
    "### Assumptions & Risks\n",
    "- CSV files are larger but universally compatible.  \n",
    "- Parquet requires external engines (`pyarrow` or `fastparquet`), which may cause compatibility issues.  \n",
    "- Website or API schema changes may affect ingested data structure.  \n",
    "- `.env` must **not** be committed to GitHub; only `.env.example` should be shared.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
